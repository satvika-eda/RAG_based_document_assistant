{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "\n",
    "# Tokenize sentences into words\n",
    "def tokenize(text):\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text.lower())\n",
    "    return text.split()\n",
    "\n",
    "# Create context-target pairs\n",
    "def generate_pairs(tokens, window_size=2):\n",
    "    pairs = []\n",
    "    for idx, target in enumerate(tokens):\n",
    "        start = max(idx - window_size, 0)\n",
    "        end = min(idx + window_size + 1, len(tokens))\n",
    "        context_words = tokens[start:idx] + tokens[idx + 1:end]\n",
    "        for context in context_words:\n",
    "            pairs.append((target, context))\n",
    "    return pairs\n",
    "\n",
    "# Reading the text data\n",
    "file_name = \"data.pdf\"\n",
    "pdf_reader = PyPDF2.PdfReader(file_name)\n",
    "text = \"\"\n",
    "for page in pdf_reader.pages[50:60]:\n",
    "    extracted_text = page.extract_text()\n",
    "    if extracted_text:\n",
    "        text += extracted_text + \"\\n\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = tokenize(text)\n",
    "\n",
    "# Function call to make the pairs\n",
    "pairs = generate_pairs(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3894"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 1029\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "vocab = Counter(chain.from_iterable([tokens]))\n",
    "word_to_id = {word: idx for idx, word in enumerate(vocab.keys())}\n",
    "id_to_word = {idx: word for word, idx in word_to_id.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Convert pairs to numerical form\n",
    "pairs_numeric = [(word_to_id[target], word_to_id[context]) for target, context in pairs]\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Embedding model\n",
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.out_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    # Forward Propagation\n",
    "    def forward(self, target):\n",
    "        embedding = self.embeddings(target)\n",
    "        output = self.out_layer(embedding)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 105365.0471\n",
      "Epoch 2/50, Loss: 97278.7964\n",
      "Epoch 3/50, Loss: 93784.7069\n",
      "Epoch 4/50, Loss: 91877.7365\n",
      "Epoch 5/50, Loss: 90779.3199\n",
      "Epoch 6/50, Loss: 90173.7671\n",
      "Epoch 7/50, Loss: 89885.8964\n",
      "Epoch 8/50, Loss: 89807.4895\n",
      "Epoch 9/50, Loss: 89872.7955\n",
      "Epoch 10/50, Loss: 90040.5818\n",
      "Epoch 11/50, Loss: 90278.9122\n",
      "Epoch 12/50, Loss: 90558.7916\n",
      "Epoch 13/50, Loss: 90851.3728\n",
      "Epoch 14/50, Loss: 91129.6333\n",
      "Epoch 15/50, Loss: 91370.4689\n",
      "Epoch 16/50, Loss: 91558.8044\n",
      "Epoch 17/50, Loss: 91689.3869\n",
      "Epoch 18/50, Loss: 91765.4392\n",
      "Epoch 19/50, Loss: 91795.3631\n",
      "Epoch 20/50, Loss: 91788.7716\n",
      "Epoch 21/50, Loss: 91754.5881\n",
      "Epoch 22/50, Loss: 91699.9562\n",
      "Epoch 23/50, Loss: 91630.2152\n",
      "Epoch 24/50, Loss: 91550.0200\n",
      "Epoch 25/50, Loss: 91462.1713\n",
      "Epoch 26/50, Loss: 91369.1791\n",
      "Epoch 27/50, Loss: 91272.6315\n",
      "Epoch 28/50, Loss: 91173.6447\n",
      "Epoch 29/50, Loss: 91073.0866\n",
      "Epoch 30/50, Loss: 90971.5203\n",
      "Epoch 31/50, Loss: 90869.3826\n",
      "Epoch 32/50, Loss: 90766.9158\n",
      "Epoch 33/50, Loss: 90664.4587\n",
      "Epoch 34/50, Loss: 90562.0122\n",
      "Epoch 35/50, Loss: 90459.5513\n",
      "Epoch 36/50, Loss: 90357.4847\n",
      "Epoch 37/50, Loss: 90255.7579\n",
      "Epoch 38/50, Loss: 90154.3630\n",
      "Epoch 39/50, Loss: 90053.4295\n",
      "Epoch 40/50, Loss: 89953.0073\n",
      "Epoch 41/50, Loss: 89853.4641\n",
      "Epoch 42/50, Loss: 89754.8705\n",
      "Epoch 43/50, Loss: 89657.5536\n",
      "Epoch 44/50, Loss: 89561.9066\n",
      "Epoch 45/50, Loss: 89468.0446\n",
      "Epoch 46/50, Loss: 89376.1462\n",
      "Epoch 47/50, Loss: 89286.3908\n",
      "Epoch 48/50, Loss: 89199.0330\n",
      "Epoch 49/50, Loss: 89114.0083\n",
      "Epoch 50/50, Loss: 89031.5334\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 50\n",
    "learning_rate = 0.0001\n",
    "epochs = 50\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = EmbeddingModel(vocab_size, embedding_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, pairs, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for target, context in pairs:\n",
    "            target_tensor = torch.tensor([target], dtype=torch.long)\n",
    "            context_tensor = torch.tensor([context], dtype=torch.long)\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            output = model(target_tensor)\n",
    "\n",
    "            # Calculate loss and backpropagate\n",
    "            loss = criterion(output, context_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "train_model(model, pairs_numeric, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved!\n"
     ]
    }
   ],
   "source": [
    "# Save embeddings to a dictionary\n",
    "embeddings = {id_to_word[idx]: model.embeddings.weight.data[idx].numpy() for idx in range(vocab_size)}\n",
    "\n",
    "# Saving the model in a pickle file\n",
    "import pickle\n",
    "with open(\"custom_embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(embeddings, f)\n",
    "\n",
    "print(\"Embeddings saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'learn': [ 0.98462373 -1.358865   -1.2448189  -0.5340973   0.9747564   0.1968791\n",
      "  0.45238492 -1.8867493   1.1169325  -0.3150388   0.51736796  0.38134807\n",
      " -0.03910092  1.3536413   0.80729246 -0.08366877  0.41622758 -0.2955528\n",
      "  0.8528182  -0.30366492  1.8349255   0.03945584 -2.5259895  -0.8204535\n",
      "  0.84046316  0.51380336  0.5347724  -0.2826849  -1.1470565  -2.7635102\n",
      " -0.6396008   0.53561383  0.27772895 -0.08337791 -1.9681256   0.39585254\n",
      " -0.13285264  0.4746073  -1.6278913  -2.314853   -1.6916685   1.3804845\n",
      "  0.85183036  0.42107642 -0.3282073   1.8663498  -1.2269049   0.29727605\n",
      "  0.33903345  0.5911918 ]\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings\n",
    "with open(\"custom_embeddings.pkl\", \"rb\") as f:\n",
    "    loaded_embeddings = pickle.load(f)\n",
    "\n",
    "# Test: Get the embedding for a word\n",
    "word = \"learn\"\n",
    "embedding = loaded_embeddings.get(word, None)\n",
    "if embedding is not None:\n",
    "    print(f\"Embedding for '{word}': {embedding}\")\n",
    "else:\n",
    "    print(f\"Word '{word}' not in vocabulary.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
