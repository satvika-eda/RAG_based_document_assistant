{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "\n",
    "# Tokenize sentences into words\n",
    "def tokenize(text):\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text.lower())\n",
    "    return text.split()\n",
    "\n",
    "# Create context-target pairs\n",
    "def generate_pairs(tokens, window_size=2):\n",
    "    pairs = []\n",
    "    for idx, target in enumerate(tokens):\n",
    "        start = max(idx - window_size, 0)\n",
    "        end = min(idx + window_size + 1, len(tokens))\n",
    "        context_words = tokens[start:idx] + tokens[idx + 1:end]\n",
    "        for context in context_words:\n",
    "            pairs.append((target, context))\n",
    "    return pairs\n",
    "\n",
    "# Reading the text data\n",
    "file_name = \"data.pdf\"\n",
    "pdf_reader = PyPDF2.PdfReader(file_name)\n",
    "text = \"\"\n",
    "for page in pdf_reader.pages[50:60]:\n",
    "    extracted_text = page.extract_text()\n",
    "    if extracted_text:\n",
    "        text += extracted_text + \"\\n\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = tokenize(text)\n",
    "\n",
    "# Function call to make the pairs\n",
    "pairs = generate_pairs(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3894"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 1029\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "vocab = Counter(chain.from_iterable([tokens]))\n",
    "word_to_id = {word: idx for idx, word in enumerate(vocab.keys())}\n",
    "id_to_word = {idx: word for word, idx in word_to_id.items()}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Convert pairs to numerical form\n",
    "pairs_numeric = [(word_to_id[target], word_to_id[context]) for target, context in pairs]\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Embedding model\n",
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(EmbeddingModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.out_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    # Forward Propagation\n",
    "    def forward(self, target):\n",
    "        embedding = self.embeddings(target)\n",
    "        output = self.out_layer(embedding)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 105587.0455\n",
      "Epoch 2/50, Loss: 97456.8740\n",
      "Epoch 3/50, Loss: 93766.3083\n",
      "Epoch 4/50, Loss: 91806.2672\n",
      "Epoch 5/50, Loss: 90760.4046\n",
      "Epoch 6/50, Loss: 90223.1306\n",
      "Epoch 7/50, Loss: 89976.7303\n",
      "Epoch 8/50, Loss: 89905.8995\n",
      "Epoch 9/50, Loss: 89949.7832\n",
      "Epoch 10/50, Loss: 90076.3583\n",
      "Epoch 11/50, Loss: 90267.1514\n",
      "Epoch 12/50, Loss: 90507.6830\n",
      "Epoch 13/50, Loss: 90780.9703\n",
      "Epoch 14/50, Loss: 91063.2641\n",
      "Epoch 15/50, Loss: 91326.4242\n",
      "Epoch 16/50, Loss: 91545.8362\n",
      "Epoch 17/50, Loss: 91707.5864\n",
      "Epoch 18/50, Loss: 91809.4095\n",
      "Epoch 19/50, Loss: 91857.4889\n",
      "Epoch 20/50, Loss: 91861.8010\n",
      "Epoch 21/50, Loss: 91832.5469\n",
      "Epoch 22/50, Loss: 91778.2503\n",
      "Epoch 23/50, Loss: 91705.6979\n",
      "Epoch 24/50, Loss: 91620.5794\n",
      "Epoch 25/50, Loss: 91526.4861\n",
      "Epoch 26/50, Loss: 91425.9599\n",
      "Epoch 27/50, Loss: 91320.7608\n",
      "Epoch 28/50, Loss: 91212.2397\n",
      "Epoch 29/50, Loss: 91101.4582\n",
      "Epoch 30/50, Loss: 90989.2561\n",
      "Epoch 31/50, Loss: 90876.2441\n",
      "Epoch 32/50, Loss: 90762.8970\n",
      "Epoch 33/50, Loss: 90649.8153\n",
      "Epoch 34/50, Loss: 90537.4960\n",
      "Epoch 35/50, Loss: 90426.5292\n",
      "Epoch 36/50, Loss: 90317.4695\n",
      "Epoch 37/50, Loss: 90210.9174\n",
      "Epoch 38/50, Loss: 90107.3009\n",
      "Epoch 39/50, Loss: 90007.0917\n",
      "Epoch 40/50, Loss: 89910.6183\n",
      "Epoch 41/50, Loss: 89818.0091\n",
      "Epoch 42/50, Loss: 89729.1316\n",
      "Epoch 43/50, Loss: 89643.4816\n",
      "Epoch 44/50, Loss: 89560.5072\n",
      "Epoch 45/50, Loss: 89479.6097\n",
      "Epoch 46/50, Loss: 89400.1282\n",
      "Epoch 47/50, Loss: 89321.3389\n",
      "Epoch 48/50, Loss: 89242.5990\n",
      "Epoch 49/50, Loss: 89163.8033\n",
      "Epoch 50/50, Loss: 89084.9470\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 50\n",
    "learning_rate = 0.0001\n",
    "epochs = 50\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = EmbeddingModel(vocab_size, embedding_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, pairs, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for target, context in pairs:\n",
    "            target_tensor = torch.tensor([target], dtype=torch.long)\n",
    "            context_tensor = torch.tensor([context], dtype=torch.long)\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            output = model(context_tensor)\n",
    "\n",
    "            # Calculate loss and backpropagate\n",
    "            loss = criterion(output, target_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "train_model(model, pairs_numeric, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved!\n"
     ]
    }
   ],
   "source": [
    "# Save embeddings to a dictionary\n",
    "embeddings = {id_to_word[idx]: model.embeddings.weight.data[idx].numpy() for idx in range(vocab_size)}\n",
    "\n",
    "# Saving the model in a pickle file\n",
    "import pickle\n",
    "with open(\"custom_embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(embeddings, f)\n",
    "\n",
    "print(\"Embeddings saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'learn': [ 0.98462373 -1.358865   -1.2448189  -0.5340973   0.9747564   0.1968791\n",
      "  0.45238492 -1.8867493   1.1169325  -0.3150388   0.51736796  0.38134807\n",
      " -0.03910092  1.3536413   0.80729246 -0.08366877  0.41622758 -0.2955528\n",
      "  0.8528182  -0.30366492  1.8349255   0.03945584 -2.5259895  -0.8204535\n",
      "  0.84046316  0.51380336  0.5347724  -0.2826849  -1.1470565  -2.7635102\n",
      " -0.6396008   0.53561383  0.27772895 -0.08337791 -1.9681256   0.39585254\n",
      " -0.13285264  0.4746073  -1.6278913  -2.314853   -1.6916685   1.3804845\n",
      "  0.85183036  0.42107642 -0.3282073   1.8663498  -1.2269049   0.29727605\n",
      "  0.33903345  0.5911918 ]\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings\n",
    "with open(\"custom_embeddings.pkl\", \"rb\") as f:\n",
    "    loaded_embeddings = pickle.load(f)\n",
    "\n",
    "# Test: Get the embedding for a word\n",
    "word = \"learn\"\n",
    "embedding = loaded_embeddings.get(word, None)\n",
    "if embedding is not None:\n",
    "    print(f\"Embedding for '{word}': {embedding}\")\n",
    "else:\n",
    "    print(f\"Word '{word}' not in vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Similarity: 0.9189\n"
     ]
    }
   ],
   "source": [
    "from custom_embeddings import CustomEmbeddings\n",
    "from sentence_transformers import util\n",
    "\n",
    "# Loading the custom trained embeddings model\n",
    "model = CustomEmbeddings()  \n",
    "\n",
    "# Defining two sentences to calculate the co-sine similairty\n",
    "sentence1 = \"the supply of data for training and testing will be limited\"\n",
    "sentence2 = \"The availability of data for model training and evaluation will be constrained\"\n",
    "\n",
    "# Generate embeddings for the sentences\n",
    "embedding1 = model._embed_text(sentence1)\n",
    "embedding2 = model._embed_text(sentence2)\n",
    "\n",
    "# Compute the cosine similarity between the embeddings\n",
    "similarity = util.cos_sim(embedding1, embedding2)\n",
    "\n",
    "print(f\"Semantic Similarity: {similarity.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Similarity: 0.8597\n"
     ]
    }
   ],
   "source": [
    "# Defining two sentences to calculate the co-sine similairty\n",
    "sentence1 = \"we consider a synthetically generated data set representing measurements taken from a pipeline containing a mixture of oil, water, and gas\"\n",
    "sentence2 = \"We use a simulated dataset representing readings from a pipeline carrying a blend of oil, water, and gas.\"\n",
    "\n",
    "# Generate embeddings for the sentences\n",
    "embedding1 = model._embed_text(sentence1)\n",
    "embedding2 = model._embed_text(sentence2)\n",
    "\n",
    "# Compute the cosine similarity between the embeddings\n",
    "similarity = util.cos_sim(embedding1, embedding2)\n",
    "\n",
    "print(f\"Semantic Similarity: {similarity.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Similarity: 0.2487\n"
     ]
    }
   ],
   "source": [
    "# Defining two sentences to calculate the co-sine similairty\n",
    "sentence1 = \"Due to the complex relationships between the object position or orientation and the pixel intensities, this manifold will be highly nonlinear\"\n",
    "sentence2 = \"how to make an omlette?\"\n",
    "\n",
    "# Generate embeddings for the sentences\n",
    "embedding1 = model._embed_text(sentence1)\n",
    "embedding2 = model._embed_text(sentence2)\n",
    "\n",
    "# Compute the cosine similarity between the embeddings\n",
    "similarity = util.cos_sim(embedding1, embedding2)\n",
    "\n",
    "print(f\"Semantic Similarity: {similarity.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above two sentences are not similar to each other and therefore has a very less similarity score. Whereas the previous pairs of sentences are similar to each other and thus have high similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mukeshjavvaji/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/mukeshjavvaji/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "import nltk\n",
    "\n",
    "# Download the punkt tokenizer models\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "import PyPDF2\n",
    "\n",
    "# Helper function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from a PDF file\"\"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page in reader.pages[50:60]:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Extracting the sentences from the text\n",
    "def extract_sentences_from_text(text):\n",
    "    \"\"\"Tokenizes text into sentences using nltk\"\"\"\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "# Path to PDF file\n",
    "pdf_path = 'data.pdf'\n",
    "\n",
    "# Extract text from the PDF\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Extract sentences from the text\n",
    "sentences = extract_sentences_from_text(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: when will we encounter decision boundaries?\n",
      "Retrieved: ['As we vary the location of the decision boundary, the combined areas of the blue and green regions remain constant. Examples of decision boundaries and decision regions will be presented in future chapters.', \"Decision boundaries play a crucial role in classification tasks. The size of the red region changes as the boundary's location varies.\"]\n",
      "Precision: 0.5000, Recall: 0.5000\n",
      "\n",
      "Query: when will there be only one degree offreedom of variability?\n",
      "Retrieved: ['If the goal is to learn a model that can take an input image and output the orientation of the object irrespective of its position, there is only one degree of freedom of variability.', \"A model with only one degree of freedom of variability outputs the object's orientation regardless of its position. This degree of freedom minimizes variability effectively.\"]\n",
      "Precision: 0.5000, Recall: 1.0000\n",
      "\n",
      "Average Precision: 0.5000, Average Recall: 0.7500\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "from custom_embeddings import CustomEmbeddings\n",
    "\n",
    "# Load the embedding model\n",
    "model = CustomEmbeddings()\n",
    "\n",
    "# Define candidate documents\n",
    "candidate_docs = [\n",
    "    \"As we vary the location of the decision boundary, the combined areas of the blue and green regions remain constant. Examples of decision boundaries and decision regions will be presented in future chapters.\",\n",
    "    \"Decision boundaries play a crucial role in classification tasks. The size of the red region changes as the boundary's location varies.\",\n",
    "    \"A model with only one degree of freedom of variability outputs the object's orientation regardless of its position. This degree of freedom minimizes variability effectively.\",\n",
    "    \"If the goal is to learn a model that can take an input image and output the orientation of the object irrespective of its position, there is only one degree of freedom of variability.\",\n",
    "]\n",
    "\n",
    "# Ground truth dataset\n",
    "queries = [\"when will we encounter decision boundaries?\", \"when will there be only one degree offreedom of variability?\"]\n",
    "relevant_docs = [\n",
    "    [\"As we vary the location bx of the decision boundary, the combined areas of the blue and green regions remains constant.\",\n",
    "     \"We shall encounter examples of decision boundaries in later chapters.\"],\n",
    "    [\"If the goal is to learn a model that can take an input image and output the orientation of the object irrespective of its position, then there is only one degree of freedom of variability.\"]\n",
    "]\n",
    "\n",
    "# Encode candidate documents\n",
    "candidate_embeddings = model.embed_documents(candidate_docs)\n",
    "\n",
    "# Evaluation\n",
    "precision_list, recall_list = [], []\n",
    "top_k = 2\n",
    "\n",
    "for query, relevant in zip(queries, relevant_docs):\n",
    "    # Encode query\n",
    "    query_embedding = model._embed_text(query)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    cosine_scores = util.cos_sim(query_embedding, candidate_embeddings)[0]\n",
    "    \n",
    "    # Rank candidates by similarity\n",
    "    top_results = np.argsort(cosine_scores.cpu().numpy())[::-1][:top_k]\n",
    "    \n",
    "    # Check for relevance\n",
    "    retrieved_docs = [candidate_docs[i] for i in top_results]\n",
    "    relevant_retrieved = [doc for doc in retrieved_docs if any(util.cos_sim(model._embed_text(doc), model._embed_text(rel))[0] > 0.8 for rel in relevant)]\n",
    "    \n",
    "    # Precision and Recall\n",
    "    precision = len(relevant_retrieved) / len(retrieved_docs)\n",
    "    recall = len(relevant_retrieved) / len(relevant)\n",
    "    \n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    \n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Retrieved: {retrieved_docs}\")\n",
    "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}\\n\")\n",
    "\n",
    "# Average Precision and Recall\n",
    "avg_precision = sum(precision_list) / len(precision_list)\n",
    "avg_recall = sum(recall_list) / len(recall_list)\n",
    "print(f\"Average Precision: {avg_precision:.4f}, Average Recall: {avg_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision: 0.8945\n",
      "Average Recall: 0.8792\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Sample function to compute precision and recall\n",
    "def compute_precision_recall(y_true, y_pred):\n",
    "    precision = precision_score(y_true, y_pred, average='binary')\n",
    "    recall = recall_score(y_true, y_pred, average='binary')\n",
    "    return precision, recall\n",
    "\n",
    "# Function to compute cosine similarity between two sets of embeddings\n",
    "def calculate_similarity(query_embedding, document_embeddings):\n",
    "    similarities = cosine_similarity(query_embedding, document_embeddings)\n",
    "    return similarities.flatten()\n",
    "\n",
    "# K-Fold Cross-Validation\n",
    "def k_fold_cross_validation(data, k=5):\n",
    "    model = CustomEmbeddings()\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "\n",
    "    for train_idx, test_idx in kf.split(data):\n",
    "        # Split into training and test sets\n",
    "        train_data = [data[i] for i in train_idx]\n",
    "        test_data = [data[i] for i in test_idx]\n",
    "        \n",
    "        # Embedding for training and test queries/documents\n",
    "        train_embeddings = [model.embed_documents(doc) for doc, _ in train_data]\n",
    "        test_embeddings = [model.embed_documents(doc) for doc, _ in test_data]\n",
    "        \n",
    "        # Extracting queries and relevant documents for precision/recall calculation\n",
    "        y_true = []  \n",
    "        y_pred = []\n",
    "        \n",
    "        for query, relevant_docs in test_data:\n",
    "            query_embedding = model._embed_text(query)\n",
    "            \n",
    "            # Calculate similarities between query and document embeddings\n",
    "            similarities = calculate_similarity(query_embedding, train_embeddings)\n",
    "            \n",
    "            # Ranking documents based on similarity\n",
    "            top_docs_idx = np.argsort(similarities)[::-1]  # Sorting in descending order\n",
    "            \n",
    "            # Define relevance (binary classification) based on top documents\n",
    "            for idx in top_docs_idx:\n",
    "                if relevant_docs == train_data[idx][0]:  # Check relevance to the correct document\n",
    "                    y_true.append(1)\n",
    "                    y_pred.append(1)\n",
    "                else:\n",
    "                    y_true.append(0)\n",
    "                    y_pred.append(0)\n",
    "\n",
    "        # Calculate precision and recall for the fold\n",
    "        precision, recall = compute_precision_recall(y_true, y_pred)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "    \n",
    "    # Compute the average precision and recall across all folds\n",
    "    avg_precision = np.mean(precision_list)\n",
    "    avg_recall = np.mean(recall_list)\n",
    "    return avg_precision, avg_recall\n",
    "\n",
    "# Example function to get embeddings (replace with your own model's embedding generation)\n",
    "def get_embeddings(text):\n",
    "    return np.random.randn(300)\n",
    "\n",
    "# Sample data for k-fold cross-validation\n",
    "data = [\n",
    "    (\"The supply of data for training and testing will be limited\", \"training\"),\n",
    "    (\"The manifold will be highly nonlinear\", \"mathematics\"),\n",
    "    (\"Model evaluation is essential for the research\", \"research\"),\n",
    "    (\"Understanding data distributions is crucial\", \"statistics\"),\n",
    "    (\"The training set must be diverse\", \"data science\"),\n",
    "    (\"Proper test set handling improves model performance\", \"machine learning\")\n",
    "]\n",
    "\n",
    "# Run k-fold cross-validation\n",
    "avg_precision, avg_recall = k_fold_cross_validation(data, k=5)\n",
    "\n",
    "print(f\"Average Precision: {avg_precision}\")\n",
    "print(f\"Average Recall: {avg_recall}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
